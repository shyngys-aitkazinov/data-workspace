{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1 Introduction to Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook presents an introduction to the PyTorch framework, as part of the material of the first exercise class in the Deep Learning course in the Autumn semester 2024. In the following, a short introduction to PyTorch data structures (tensors) will be made, along with practical examples involving all the components necessary for building, training and testing PyTorch models: starting from the module classes, optimizers, losses, datasets and dataloaders, covering the basics of backpropagation as well. Following the PyTorch introduction, an example of a Perceptron training is presented."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 Tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tensors are the main datatype used in the PyTorch framework. Much like numpy arrays, they hold numerical values in arrays of different shapes. Not only that, but in case of mathematical oprations, tensors behave exactly the same as numpy arrays - they even share the same memory space if constructed from one another (when creating an np.array from torch.tensor or vice versa, both object reference the same underlying storage in memory)!\n",
        "\n",
        "Tensors come with some additional features on top of the numpy arrays - the main feature being that they contain **gradient information**! \n",
        "PyTorch tensors are designed to be used in the context of gradient descent optimization, and therefore they hold not only a tensor with numeric values, but also **the computational graph** leading to these values.\n",
        "\n",
        "In the following example, we will create some tensors and show how to compute the gradients with respect to any variable needed - this is a crucial part of neural network training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 1: Tensor gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = torch.tensor([1., 2., 3.], requires_grad=True) # Input vector\n",
        "w = torch.tensor([4., 5., 1.], requires_grad=True) # Weight vecotr\n",
        "t = torch.tensor([2], requires_grad=False) # Target values\n",
        "\n",
        "y = w @ x # inner-product of x and w\n",
        "z = (y - t)**2 # square error between the output and target values\n",
        "\n",
        "z.backward()  # ask pytorch to trace back the computation of z and compute derivatives\n",
        "\n",
        "# TODO: Compute the gradient of variable z with respect to the weight vector w, and the input vector x\n",
        "dzdw = ...\n",
        "dzdx = ...\n",
        "\n",
        "print(f\"Gradient computed automatically by pytorch: {w.grad}\")  # the resulting gradient of z w.r.t w -> computed using backward()\n",
        "print(f\"Gradient computed manually:                 {dzdw}\")\n",
        "\n",
        "print(f\"Gradient computed automatically by pytorch: {x.grad}\")  # the resulting gradient of z w.r.t \n",
        "print(f\"Gradient computed manually:                 {dzdx}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1.2 PyTorch models and datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.datasets import make_regression, make_blobs\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "%config InlineBackend.figure_format = 'svg'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "X, y_true = make_regression(n_samples=60, n_features=10, noise=1.)\n",
        "X_tensor, y_tensor = torch.tensor(X, dtype=torch.float32), torch.tensor(y_true, dtype=torch.float32)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor.unsqueeze(1), test_size=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2.1 Writing custom datasets:\n",
        "- A Dataset subclass wraps access to the data, and is specialized to the type of data it’s serving.\n",
        "- The DataLoader knows nothing about the data, but organizes the input tensors served by the Dataset into batches with the parameters you specify.\n",
        "\n",
        "torch.utils.data.Dataset is an abstract class representing a dataset. Your custom dataset should inherit Dataset and override the methods \\__len\\__ and \\__getitem\\__:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, X, y, transform=None):\n",
        "        \"\"\"\n",
        "        X is assumed to take the shape [num_samples, embedding_dim]\n",
        "        y has the shape [num_samples,]\n",
        "        \"\"\"\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        # TODO: Implement the __len__ method for the dataset of vector values\n",
        "        pass\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # TODO: Implement the __getitem__ method for the dataset of vector values applying the transformation if possible\n",
        "        pass\n",
        "    \n",
        "class MyTransform(object):\n",
        "    \"\"\"\n",
        "    Example of a custom transform that is applied to every element of the Dataset class before they are returned using the __getitem__ method.\n",
        "    There's no need to modify this class - it serves just as an example of how tranforms can be made and applied during training of neural networks.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def __call__(self, X, y):\n",
        "        return (X-X.mean(axis=0))/X.std(axis=0), (y-y.mean(axis=0))/y.std(axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1.2.2 Training a neural network steps:\n",
        "**The loss function** is a measure of how far from our ideal output the model’s prediction was. Mean-squared-error loss is a typical loss function for regression models like ours.\n",
        "**The optimizer** is what drives the learning. Here we use an optimizer that implements stochastic gradient descent. Besides parameters of the algorithm, like the learning rate and momentum, we also pass in net.parameters() - a collection of all the learning weights in the model - which is what the optimizer adjusts.\n",
        "**Zeroing the gradients** is an important step. Gradients are accumulated over a batch. If we do not reset them for every batch, they will keep accumulating, which will provide incorrect gradient values, making learning impossible.\n",
        "\n",
        "Usual structure of a training loop:\n",
        "- Get the outputs of the current batch by passing it through the network\n",
        "- Compute the loss (Ground truth labels vs the network output)\n",
        "- Perform a backwards pass of the network (calculates the gradients of loss wrt networks parameters)\n",
        "- Perform an optimizer step: why this is decoupled from the backwards step is because the optimizer can be abstracted, as it can implement many different algorithms of optimization, all of which require backwards derivatives "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, train_loader, optimizer, criterion, device):\n",
        "    \"\"\"\n",
        "    Implements one epoch (one whole pass through the dataset) of training the provided model.\n",
        "    \"\"\"\n",
        "    epoch_loss = 0\n",
        "\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        # TODO: Finish the training loop by performing the necessary operations using the optimizer, model and criterion objects.\n",
        "        #       Accumulate each batch's loss in the variable epoch_loss.\n",
        "        \n",
        "    \n",
        "    return epoch_loss/len(train_loader)\n",
        "\n",
        "def evaluate(model, val_loader, criterion, device):\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    val_loss = 0\n",
        "    for x, y in val_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.no_grad():\n",
        "            val_loss += criterion(model(x), y).item()\n",
        "\n",
        "    return val_loss/len(val_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note:** The loss function is not independent from the optimizer! It is the final leaf in a single computational graph which starts with the model inputs and contains all model parameters. When we do loss.backward() the process of backpropagation starts at the loss and goes through all of its parents all the way to model inputs. All nodes in the graph contain a reference to their parent.\n",
        "\n",
        "loss.backward() computes the grad attribute of all tensors with requires_grad=True in the computational graph of which loss is the leaf.\n",
        "\n",
        "Optimizer just iterates through the list of parameters which have requires_grad=True set, which it received on initialization, and it subtracts the value of its gradient stored in its .grad property, simply multiplied by the learning rate in case of SGD. It doesn't need to know with respect to what loss the gradients were computed it just wants to access that .grad property. The gradients are \"stored\" by the tensors themselves (they have a grad and a requires_grad attributes) once you call backward() on the loss. After computing the gradients for all tensors in the model, calling optimizer.step() makes the optimizer iterate over all parameters (tensors) it is supposed to update and use their internally stored grad to update their values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2.3 Writing custom models:\n",
        "Some facts and tips about writing a custom PyTorch model:\n",
        "- It **inherits from torch.nn.Module** - modules may be nested.\n",
        "- A model should have an **__init__() function**, where it instantiates its layers, and loads any data artifacts it might need (e.g., an NLP model might load a vocabulary).\n",
        "- A model should have a **forward()** function. This is where the actual computation happens: An input is passed through the network layers and various functions to generate an output.\n",
        "- Other than that, you can build out your model class like any other Python class, adding whatever properties and methods you need to support your model’s computation.\n",
        "- PyTorch models assume they are working on batches of data\n",
        "- Inference is performed by calling it like a function: model(input)\n",
        "- Output of a model also has a batch dimension, the size of which should always match the input batch dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_fc = nn.Linear(input_dim, 10)\n",
        "        self.hidden_fc = nn.Linear(10, 10)\n",
        "        self.output_fc = nn.Linear(10, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: Write the forward method of the MLP, using a suitable activation function.\n",
        "        #       The output variable should be named y_pred.\n",
        "\n",
        "        return y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The below code uses a simple linear model as baseline.\n",
        "If you run just this you should be able to see how the model performance improves and in the next cell you can plot your predictions against the actual values.\n",
        "Test your own model by uncommenting the line below. What do you observe and can you fix it somehow? How does your choice of activation function affect the model performance and why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs = 40\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = torch.nn.Sequential(torch.nn.Linear(10, 1))\n",
        "# model = MLP(10, 1)\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "used_transformation = None # MyTransform()\n",
        "train_dataset = MyDataset(X_train, y_train, transform=used_transformation)\n",
        "val_dataset = MyDataset(X_test, y_test, transform=used_transformation)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=10)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "model.to(device)\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    epoch_loss_train = train(model, train_loader, optimizer, criterion, device)\n",
        "    train_loss.append(epoch_loss_train)\n",
        "    \n",
        "    epoch_loss_val = evaluate(model, val_loader, criterion, device)\n",
        "    val_loss.append(epoch_loss_val)\n",
        "\n",
        "    print(f'epoch {epoch+1}, loss {epoch_loss_val:.2f}')\n",
        "\n",
        "plt.plot(range(epochs), train_loss, label='Training loss')\n",
        "plt.plot(range(epochs), val_loss, label='Validation loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    pred = model(X_test.cuda()).cpu().numpy()\n",
        "\n",
        "if used_transformation:\n",
        "    pred = pred * y_train.std().item() + y_train.mean().item()\n",
        "\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r')\n",
        "plt.scatter(y_test.numpy(), pred)\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2 The perceptron"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "%config InlineBackend.figure_format = 'svg'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Random dataset with two blobs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X, y = make_blobs(n_samples=50, centers=2, random_state=4)\n",
        "\n",
        "X = preprocessing.scale(X)\n",
        "# X[y==0] += 1 # linearly non-separable data\n",
        "\n",
        "y[y==0]=-1\n",
        "plt.scatter(X[:,0], X[:,1], c=y)\n",
        "plt.title(\"Dataset\")\n",
        "plt.gca().set_aspect('equal', adjustable='box')\n",
        "plt.xlabel(r'$x_1$')\n",
        "plt.ylabel(r'$x_2$')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# split train and test data\n",
        "y_true = y\n",
        "\n",
        "# add a dimension for bias\n",
        "X = np.hstack((X, np.ones((X.shape[0], 1))))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_true, stratify=y)\n",
        "print(f'Shape X_train: {X_train.shape}')\n",
        "print(f'Shape y_train: {y_train.shape}')\n",
        "print(f'Shape X_test: {X_test.shape}')\n",
        "print(f'Shape y_test: {y_test.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Perceptron class + training algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Perceptron():\n",
        "    \n",
        "    def __init__(self, n_samples, n_features, lr=1., n_iters=1000):\n",
        "        self.lr = lr\n",
        "        self.n_samples = n_samples\n",
        "        self.n_features = n_features\n",
        "        self.n_iters = n_iters\n",
        "        self.theta_hist = np.zeros((self.n_features, self.n_iters))\n",
        "        self._trained = False\n",
        "        self.train_accuracies = []\n",
        "        self.test_accuracies = []\n",
        "\n",
        "        \n",
        "    def train(self, X_train, y_train, X_test, y_test):\n",
        "        \n",
        "        theta = np.random.uniform(size=(self.n_features,))\n",
        "        \n",
        "        for i in tqdm(range(self.n_iters)):\n",
        "            idx = i % self.n_samples\n",
        "            #TODO: implement the prediction for X_train[idx,:].\n",
        "            y_predict = ...\n",
        "\n",
        "            #TODO: implement the update rule for the preceptron by updating theta.\n",
        "\n",
        "            self.theta_hist[:,i] = theta\n",
        "\n",
        "            # computes train accuracy\n",
        "            y_train_predict = np.sign(np.inner(X_train, theta))\n",
        "            self.train_accuracies.append(np.mean(y_train_predict == y_train))\n",
        "\n",
        "            # computes test accuracy\n",
        "            y_test_predict = np.sign(np.inner(X_test, theta))\n",
        "            self.test_accuracies.append(np.mean(y_test_predict == y_test))\n",
        "        \n",
        "        self._trained = True\n",
        "        \n",
        "\n",
        "    def is_trained(self):\n",
        "        if not self._trained:\n",
        "            raise ValueError(\"Model has not been trained.\")\n",
        "\n",
        "    @property\n",
        "    def theta(self):\n",
        "        self.is_trained()\n",
        "        return self.theta_hist[:, -1]\n",
        "    \n",
        "    @property\n",
        "    def pocket_idx(self):\n",
        "        self.is_trained()\n",
        "        return np.argmax(self.train_accuracies)\n",
        "    \n",
        "    @property\n",
        "    def pocket_theta(self):\n",
        "        self.is_trained()\n",
        "        return self.theta_hist[:, self.pocket_idx]\n",
        "        \n",
        "        \n",
        "p = Perceptron(*X_train.shape)\n",
        "theta = p.train(X_train, y_train, X_test, y_test)\n",
        "\n",
        "# pocket: take the theta with the best training accuracy.\n",
        "print(f'\\ntheta: {p.theta}')\n",
        "print(f'pocket_theta: {p.pocket_theta}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Plotting training curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(p.train_accuracies, label='train acc')\n",
        "plt.plot(p.test_accuracies, label='test acc')\n",
        "plt.scatter(p.pocket_idx, p.train_accuracies[p.pocket_idx], c='k', marker='*', label='pocket', s=100)\n",
        "plt.ylim([0.3, 1.05])\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "\n",
        "print(f'Train accuracy last: {p.train_accuracies[-1]}')\n",
        "print(f'Test accuracy last: {p.test_accuracies[-1]}')\n",
        "print('############################################')\n",
        "print(f'Train accuracy pocket: {p.train_accuracies[p.pocket_idx]}')\n",
        "print(f'Test accuracy pocket: {p.test_accuracies[p.pocket_idx]}')\n",
        "print('########## Thetas ##############')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4 Visualization of training (parameter space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "theta_history = p.theta_hist\n",
        "\n",
        "plt.plot(theta_history[0,:], theta_history[1,:], 'b', lw=2)\n",
        "plt.scatter(theta_history[0,:], theta_history[1,:], facecolor='b', s=30)\n",
        "plt.scatter(theta_history[0,0], theta_history[1,0], facecolor='r', s=70)\n",
        "plt.scatter(theta_history[0,-1], theta_history[1,-1], facecolor='g', s=70)\n",
        "\n",
        "plt.grid()\n",
        "plt.gca().set_aspect('equal', adjustable='box')\n",
        "plt.xlabel(r'$\\bf \\theta$, first coordinate')\n",
        "plt.ylabel(r'$\\bf \\theta$, second coordinate')\n",
        "plt.title('trajectory starts from red point, ends at green point')\n",
        "plt.savefig('traj.png',dpi = 400)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.5 Visualization of solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.max(np.abs(X[:,0]))\n",
        "\n",
        "fig = plt.figure(figsize=(8,6))\n",
        "plt.scatter(X_train[:,0], X_train[:,1], c=y_train)\n",
        "plt.scatter(X_test[:,0], X_test[:,1], c=y_test, marker='x')\n",
        "x_hyperplane = np.array([np.min(X[:,0]),np.max(X[:,0])])\n",
        "y_hyperplane = -(x_hyperplane * p.theta[0] + p.theta[2]) / p.theta[1]\n",
        "plt.plot(x_hyperplane, y_hyperplane, '-', label='last theta')\n",
        "\n",
        "# pocket solution\n",
        "y_hyperplane = -(x_hyperplane * p.pocket_theta[0] + p.pocket_theta[2]) / p.pocket_theta[1]\n",
        "plt.plot(x_hyperplane, y_hyperplane, '-', label='pocket theta')\n",
        "\n",
        "plt.title(\"Dataset\")\n",
        "plt.gca().set_aspect('equal', adjustable='box')\n",
        "plt.xlabel(r'$\\bf x$, first coordinate')\n",
        "plt.ylabel(r'$\\bf x$, second coordinate')\n",
        "print(f'theta: {p.theta}')\n",
        "print(f'pocket_theta: {p.pocket_theta}')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.6 Perceptron with Real World Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_digits\n",
        "\n",
        "X, y = load_digits(n_class=2, return_X_y=True)\n",
        "y[y==0] = -1.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "scaler = preprocessing.StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "p = Perceptron(*X_train.shape, n_iters=50)\n",
        "p.train(X_train, y_train, X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(p.train_accuracies, label='train acc')\n",
        "plt.plot(p.test_accuracies, label='test acc')\n",
        "plt.scatter(p.pocket_idx, p.train_accuracies[p.pocket_idx], c='k', marker='*', label='pocket', s=100)\n",
        "plt.ylim([0.3, 1.05])\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "\n",
        "print(f'Train accuracy last: {p.train_accuracies[-1]}')\n",
        "print(f'Test accuracy last: {p.test_accuracies[-1]}')\n",
        "print('############################################')\n",
        "print(f'Train accuracy pocket: {p.train_accuracies[p.pocket_idx]}')\n",
        "print(f'Test accuracy pocket: {p.test_accuracies[p.pocket_idx]}')\n",
        "print('########## Thetas ##############')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
