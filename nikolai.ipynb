{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prelim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "\n",
    "# depending on your IDE, you might need to add datathon_eth. in front of data\n",
    "from data import DataLoader, SimpleEncoding\n",
    "\n",
    "# depending on your IDE, you might need to add datathon_eth. in front of forecast_models\n",
    "from forecast_models import SimpleModel\n",
    "\n",
    "country = \"IT\" \n",
    "\"\"\"\n",
    "\n",
    "Train and evaluate the models for IT and ES\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Inputs\n",
    "input_path = r\"datasets2025\"\n",
    "output_path = r\"outputs\"\n",
    "\n",
    "# Load Datasets\n",
    "loader = DataLoader(input_path)\n",
    "# features are holidays and temperature\n",
    "training_set, features, example_results = loader.load_data(country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set[training_set.columns[4]][10000:10200].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEncoding:\n",
    "    \"\"\"\n",
    "    This class is an example of dataset encoding.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        consumption: pd.Series,\n",
    "        features: pd.Series,\n",
    "        end_training,\n",
    "        start_forecast,\n",
    "        end_forecast,\n",
    "    ):\n",
    "        self.consumption_mask = ~consumption.isna()\n",
    "        self.consumption = consumption[self.consumption_mask]\n",
    "        self.features = features\n",
    "        self.end_training = end_training\n",
    "        self.start_forecast = start_forecast\n",
    "        self.end_forecast = end_forecast\n",
    "\n",
    "    def meta_encoding(self):\n",
    "        \"\"\"\n",
    "        This function returns the feature, split between past (for training) and future (for forecasting)),\n",
    "        as well as the consumption, without missing values.\n",
    "        :return: three numpy arrays\n",
    "\n",
    "        \"\"\"\n",
    "        features_past = self.features[: self.end_training].values.reshape(-1, 1)\n",
    "        features_future = self.features[\n",
    "            self.start_forecast : self.end_forecast\n",
    "        ].values.reshape(-1, 1)\n",
    "\n",
    "        features_past = features_past[self.consumption_mask]\n",
    "\n",
    "        return features_past, features_future, self.consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set[training_set.columns[0]].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set[training_set.columns[0]].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class Encoder:\n",
    "    \"\"\"\n",
    "    This class is an example of dataset encoding for a multivariate feature set.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        consumption: pd.Series,\n",
    "        features: pd.DataFrame,\n",
    "        end_training,\n",
    "        start_forecast,\n",
    "        end_forecast,\n",
    "    ):\n",
    "        # assert isinstance(features, pd.DataFrame), \"features must be a DataFrame\"\n",
    "        self.consumption_mask = ~consumption.isna()\n",
    "        self.consumption = consumption[self.consumption_mask]\n",
    "        self.features = features\n",
    "        self.end_training = end_training\n",
    "        self.start_forecast = start_forecast\n",
    "        self.end_forecast = end_forecast\n",
    "\n",
    "    def meta_encoding(self):\n",
    "        \"\"\"\n",
    "        This function returns the feature matrix split between past (for training)\n",
    "        and future (for forecasting), as well as the non-missing consumption values.\n",
    "        :return: (features_past, features_future, consumption) as numpy arrays\n",
    "        \"\"\"\n",
    "        # Select past and future feature slices\n",
    "        features_past = self.features.loc[:self.end_training].values\n",
    "        features_future = self.features.loc[self.start_forecast:self.end_forecast].values\n",
    "\n",
    "        # Apply the consumption mask to past features\n",
    "        features_past = features_past[self.consumption_mask]\n",
    "\n",
    "        return features_past, features_future, self.consumption.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = r\"datasets2025\"\n",
    "output_path = r\"outputs\"\n",
    "\n",
    "# Load Datasets\n",
    "loader = DataLoader(input_path)\n",
    "# features are holidays and temperature\n",
    "zone = \"IT\" # \"ES\"\n",
    "training_set, features, example_results = loader.load_data(zone)\n",
    "start_training = training_set.index.min()\n",
    "end_training = training_set.index.max()\n",
    "start_forecast, end_forecast = example_results.index[0], example_results.index[-1]\n",
    "consumption = training_set\n",
    "\n",
    "feature_dummy = features['temp'].loc[start_training:]\n",
    "\n",
    "# encoding = Encoder(\n",
    "#     consumption, feature_dummy, end_training, start_forecast, end_forecast\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.iloc[:, 0:5].pct_change().corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.iloc[-2900:-2700, 0:5].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rollout_it = pd.read_csv(\"datasets2025/rollout_data_IT.csv\", index_col = 0, parse_dates = True)\n",
    "# rollout_it.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customer_id = 1\n",
    "# joint_df = pd.concat([rollout_it.loc[:, rollout_it.columns[customer_id]], \n",
    "#                     training_set.loc[:, training_set.columns[customer_id]]],\n",
    "#                     axis = 1)\n",
    "# joint_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joint_df.iloc[10800:11000].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourier Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer = training_set.columns[0]\n",
    "X = training_set[customer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# --------- Model Classes ---------\n",
    "\n",
    "class BaseModel:\n",
    "    def train(self, X, y):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict(self, X):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class LightGBMModel(BaseModel):\n",
    "    def __init__(self, **params):\n",
    "        self.model = LGBMRegressor(**params)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "class LinearRegressionModel(BaseModel):\n",
    "    def __init__(self, **params):\n",
    "        self.model = LinearRegression(**params)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# --------- Time Feature Extractor ---------\n",
    "\n",
    "class TimeFeaturesExtractor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Expecting X to be a DataFrame with a DatetimeIndex.\n",
    "        df = pd.DataFrame(index=X.index)\n",
    "        df[\"hour\"] = X.index.hour\n",
    "        df[\"dayofweek\"] = X.index.dayofweek\n",
    "        return df\n",
    "\n",
    "# --------- Main Forecast Function ---------\n",
    "\n",
    "def forecast_time_series(\n",
    "    X: pd.Series,\n",
    "    forecast_horizon: int = 720,  # forecast 720 time steps which are the test set\n",
    "    model_type: str = \"lightgbm\",\n",
    "    **model_params\n",
    "):\n",
    "    # Ensure the index is a DatetimeIndex and drop missing values\n",
    "    if not isinstance(X.index, pd.DatetimeIndex):\n",
    "        raise ValueError(\"The time series index must be a pandas DatetimeIndex.\")\n",
    "    X = X.dropna()\n",
    "\n",
    "    # Split data into training and test sets\n",
    "    if len(X) < forecast_horizon:\n",
    "        raise ValueError(\"The time series does not have enough data for the specified forecast horizon.\")\n",
    "        \n",
    "    train_size = len(X) - forecast_horizon\n",
    "    X_train_raw = X.iloc[:train_size]\n",
    "    X_test_raw = X.iloc[train_size:]\n",
    "    \n",
    "    y_train = X_train_raw.values\n",
    "    y_test = X_test_raw.values\n",
    "\n",
    "    # Feature pipeline: extract time features and then one-hot encode them.\n",
    "    feature_extractor = Pipeline([\n",
    "        ('time_features', TimeFeaturesExtractor()),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    # Fit the feature extractor on the training data and transform both sets\n",
    "    X_train_features = feature_extractor.fit_transform(X_train_raw.to_frame())\n",
    "    X_test_features = feature_extractor.transform(X_test_raw.to_frame())\n",
    "\n",
    "    # Choose the forecasting model\n",
    "    model_classes = {\n",
    "        \"lightgbm\": LightGBMModel,\n",
    "        \"linear\": LinearRegressionModel,\n",
    "    }\n",
    "    if model_type not in model_classes:\n",
    "        raise ValueError(f\"Unsupported model type: {model_type}. Choose from {list(model_classes.keys())}\")\n",
    "\n",
    "    model = model_classes[model_type](**model_params)\n",
    "    model.train(X_train_features, y_train)\n",
    "\n",
    "    # Predict the test set in one go\n",
    "    y_pred = model.predict(X_test_features)\n",
    "\n",
    "    # Calculate and print Mean Squared Error for the test set\n",
    "    mse = np.mean(np.square(y_pred - y_test))\n",
    "    print(\"Test MSE:\", mse)\n",
    "\n",
    "    # Plot ground truth vs. predicted values for the 720-step test set\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(X_test_raw.index, y_test, label='Ground Truth', linewidth=2)\n",
    "    plt.plot(X_test_raw.index, y_pred, label='Predicted', linestyle='--')\n",
    "    plt.title(f\"{forecast_horizon}-Step Forecast Comparison ({model_type})\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Optionally, return a DataFrame with the ground truth and predicted values\n",
    "    results = pd.DataFrame({\n",
    "        \"Ground Truth\": y_test,\n",
    "        \"Predicted\": y_pred\n",
    "    }, index=X_test_raw.index)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_time_series(X, 720, model_type=\"lightgbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_time_series(X, 720, model_type=\"linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBF Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "\n",
    "# --------- Model Classes ---------\n",
    "\n",
    "class BaseModel:\n",
    "    def train(self, X, y):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict(self, X):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class LightGBMModel(BaseModel):\n",
    "    def __init__(self, **params):\n",
    "        self.model = LGBMRegressor(**params)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "\n",
    "class LinearRegressionModel(BaseModel):\n",
    "    def __init__(self, **params):\n",
    "        self.model = LinearRegression(**params)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "\n",
    "# --------- Time Feature Extractor ---------\n",
    "\n",
    "class TimeFeaturesExtractor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        df = pd.DataFrame(index=X.index)\n",
    "        df[\"hour\"] = X.index.hour\n",
    "        df[\"dayofweek\"] = X.index.dayofweek\n",
    "        return df\n",
    "\n",
    "\n",
    "# --------- Main Forecast Function ---------\n",
    "\n",
    "def forecast_time_series(\n",
    "    X: pd.Series,\n",
    "    forecast_horizon: int = 240,\n",
    "    test_frac: float = 0.2,\n",
    "    model_type: str = \"lightgbm\",\n",
    "    **model_params\n",
    "):\n",
    "    # Drop missing values\n",
    "    X = X.dropna()\n",
    "\n",
    "    # Ensure enough data for test\n",
    "    total_len = len(X)\n",
    "    test_size = max(int(total_len * test_frac), forecast_horizon)\n",
    "    train_size = total_len - test_size\n",
    "\n",
    "    X_train_raw = X.iloc[:train_size]\n",
    "    X_test_raw = X.iloc[train_size:train_size + forecast_horizon]\n",
    "    y_train = X_train_raw.values\n",
    "    y_test = X_test_raw.values\n",
    "\n",
    "    # Feature pipeline\n",
    "    feature_extractor = Pipeline([\n",
    "        ('time_features', TimeFeaturesExtractor()),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    X_train_features = feature_extractor.fit_transform(X_train_raw.to_frame())\n",
    "    X_test_features = feature_extractor.transform(X_test_raw.to_frame())\n",
    "\n",
    "    # Choose model\n",
    "    model_classes = {\n",
    "        \"lightgbm\": LightGBMModel,\n",
    "        \"linear\": LinearRegressionModel,\n",
    "    }\n",
    "\n",
    "    if model_type not in model_classes:\n",
    "        raise ValueError(f\"Unsupported model type: {model_type}. Choose from {list(model_classes.keys())}\")\n",
    "\n",
    "    model = model_classes[model_type](**model_params)\n",
    "    model.train(X_train_features, y_train)\n",
    "    y_pred = model.predict(X_test_features)\n",
    "\n",
    "    print(\"MSE:\", np.mean(np.square(y_pred - y_test)))\n",
    "\n",
    "    # Plot predictions\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(X_test_raw.index, y_test, label='Ground Truth', linewidth=2)\n",
    "    plt.plot(X_test_raw.index, y_pred, label='Forecast', linestyle='--')\n",
    "    plt.title(f\"{forecast_horizon}-Step Forecast ({model_type})\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# LightGBM\n",
    "forecast_time_series(X, model_type=\"lightgbm\", forecast_horizon=720, n_estimators=100, learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_time_series(X, model_type=\"linear\", forecast_horizon=720)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "\n",
    "# depending on your IDE, you might need to add datathon_eth. in front of data\n",
    "from data import DataLoader, SimpleEncoding\n",
    "\n",
    "# depending on your IDE, you might need to add datathon_eth. in front of forecast_models\n",
    "from forecast_models import SimpleModel\n",
    "\n",
    "country = \"ES\" \n",
    "\"\"\"\n",
    "\n",
    "Train and evaluate the models for IT and ES\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Inputs\n",
    "input_path = r\"datasets2025\"\n",
    "output_path = r\"outputs\"\n",
    "\n",
    "# Load Datasets\n",
    "loader = DataLoader(input_path)\n",
    "# features are holidays and temperature\n",
    "training_set, features, example_results = loader.load_data(country)\n",
    "rollout, holidays = loader.load_additional_data(country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import DataLoader, DatasetEncoding\n",
    "\n",
    "team_name = \"HANGUK_ML\"\n",
    "# Data Manipulation and Training\n",
    "start_training = training_set.index.min()\n",
    "end_training = training_set.index.max()\n",
    "start_forecast, end_forecast = example_results.index[0], example_results.index[-1]\n",
    "\n",
    "range_forecast = pd.date_range(start=start_forecast, end=end_forecast, freq=\"1H\")\n",
    "\n",
    "de = DatasetEncoding(\n",
    "    consumption=training_set,\n",
    "    features=features,\n",
    "    rollout=rollout,\n",
    "    holiday=holidays,\n",
    "    end_training=end_training,\n",
    "    start_forecast=start_forecast,\n",
    "    end_forecast=end_forecast,\n",
    ")\n",
    "df = de.generate_time_series_features(\n",
    "    start_time=start_training,\n",
    "    end_time=end_training,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = de.generate_dataset(customer_id=1, window_size=24 * 7, forecast_skip=1, forecast_horizon=7)\n",
    "df_train = df[:-720]\n",
    "df_test = df[-720:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## naive1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseline_models import naive\n",
    "\n",
    "model_naive = naive(prediction_window=720)\n",
    "pred = model_naive.predict(df_train)\n",
    "pred.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_df = pd.concat([df_test[\"consumption\"], pred], axis = 1)\n",
    "print(\"MSE = \", np.mean(np.square(df_test[\"consumption\"] - pred)))\n",
    "joint_df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## naive2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from baseline_models import naive2\n",
    "\n",
    "# model_naive = naive2(prediction_window=737)\n",
    "# model_naive.train(df_train)\n",
    "# pred = model_naive.predict(df_train)\n",
    "# pred.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joint_df = pd.concat([df_test[\"consumption\"], pred], axis = 1)\n",
    "# print(\"MSE = \", np.mean(np.square(df_test[\"consumption\"] - pred)))\n",
    "# joint_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joint_df = pd.concat([df_test[\"consumption\"], pred], axis = 1)\n",
    "# print(\"MSE = \", np.mean(np.square(df_test[\"consumption\"] - pred)))\n",
    "# joint_df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## naive_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class plain:\n",
    "\n",
    "    def __init__(self, prediction_window: int = 720):\n",
    "        # prediction_window is the number of future hourly time steps to forecast.\n",
    "        self.prediction_window = prediction_window\n",
    "\n",
    "    def train(self, **kwargs):\n",
    "        # No training is needed for a naive predictor.\n",
    "        pass\n",
    "\n",
    "    def predict(self, x: pd.Series) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Predict the next prediction_window timesteps after the last index in x.\n",
    "\n",
    "        The procedure is:\n",
    "          - Define a prediction range: hourly timestamps starting from the next midnight after\n",
    "            the last observation.\n",
    "          - Candidate one: lookup the corresponding period one year ago.\n",
    "          - Candidate two: lookup the corresponding period one month ago.\n",
    "          - For each timestep in the prediction range, if both candidate values are available,\n",
    "            take their average; if only one is available, use that value; if both are missing,\n",
    "            return 0.\n",
    "        \n",
    "        Parameters:\n",
    "          x (pd.Series): Time series with a DatetimeIndex.\n",
    "        \n",
    "        Returns:\n",
    "          pd.Series: Forecasted values for the next prediction_window hours, with the future timestamps.\n",
    "        \"\"\"\n",
    "        # Ensure x has a DatetimeIndex\n",
    "        if not isinstance(x.index, pd.DatetimeIndex):\n",
    "            raise ValueError(\"Input time series must have a DatetimeIndex.\")\n",
    "\n",
    "        x = x['consumption']\n",
    "\n",
    "        # Determine the current observation (last timestamp)\n",
    "        current_observation = x.index[-1]\n",
    "        # Determine the next day’s midnight:\n",
    "        prediction_start = (current_observation + pd.Timedelta(days=1)).normalize()\n",
    "        # Create the prediction range with hourly frequency\n",
    "        prediction_range = pd.date_range(start=prediction_start, periods=self.prediction_window, freq='H')\n",
    "\n",
    "        candidate_range_year = prediction_range - DateOffset(years=1)\n",
    "        candidate_range_month = prediction_range - DateOffset(months=1)\n",
    "\n",
    "        candidate_one = x.reindex(candidate_range_year)\n",
    "        candidate_one.index = prediction_range  # Carry candidate values into the future index\n",
    "\n",
    "        candidate_two = x.reindex(candidate_range_month)\n",
    "        candidate_two.index = prediction_range  # Carry candidate values into the future index\n",
    "        \n",
    "        df_candidates = pd.DataFrame({\n",
    "            \"candidate_one\": candidate_one,\n",
    "            \"candidate_two\": candidate_two\n",
    "        }, index=prediction_range)\n",
    "\n",
    "        forecast = df_candidates.mean(axis=1, skipna=True)\n",
    "        forecast = forecast.fillna(0)\n",
    "\n",
    "        # Debug prints (optional)\n",
    "        print(\"Current observation:\", current_observation)\n",
    "        print(\"Prediction range starts at:\", prediction_range[0])\n",
    "        print(\"Candidate range (1 year ago) starts at:\", candidate_range_year[0])\n",
    "        print(\"Candidate range (1 month ago) starts at:\", candidate_range_month[0])\n",
    "\n",
    "        return forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "\n",
    "# depending on your IDE, you might need to add datathon_eth. in front of data\n",
    "from data import DataLoader, SimpleEncoding\n",
    "\n",
    "# depending on your IDE, you might need to add datathon_eth. in front of forecast_models\n",
    "from forecast_models import SimpleModel\n",
    "\n",
    "country = \"IT\" \n",
    "\"\"\"\n",
    "\n",
    "Train and evaluate the models for IT and ES\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Inputs\n",
    "input_path = r\"datasets2025\"\n",
    "output_path = r\"outputs\"\n",
    "\n",
    "# Load Datasets\n",
    "loader = DataLoader(input_path)\n",
    "# features are holidays and temperature\n",
    "training_set, features, example_results = loader.load_data(country)\n",
    "rollout, holidays = loader.load_additional_data(country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'training_set' is your DataFrame with a datetime index\n",
    "# and each column corresponds to a different customer.\n",
    "\n",
    "# Step 1: Compute basic features for each customer\n",
    "\n",
    "# Basic statistics computed on the raw time series\n",
    "features = pd.DataFrame(index=training_set.columns)\n",
    "features['mean'] = training_set.mean()\n",
    "features['std'] = training_set.std()\n",
    "features['max'] = training_set.max()\n",
    "features['min'] = training_set.min()\n",
    "\n",
    "# Optionally, resample to daily frequency to extract daily consumption features.\n",
    "daily_data = training_set.resample('D').mean()\n",
    "\n",
    "# Compute daily-based features\n",
    "features['daily_mean'] = daily_data.mean()\n",
    "features['daily_std'] = daily_data.std()\n",
    "\n",
    "# You can further compute additional features such as:\n",
    "# - The percentage of missing values per customer\n",
    "# - Autocorrelation coefficients (e.g., lag-1 autocorrelation)\n",
    "features['missing_rate'] = training_set.isna().mean()\n",
    "\n",
    "# Example: Calculate lag-1 autocorrelation for each customer\n",
    "def lag1_autocorr(x):\n",
    "    x = x.dropna()  # drop NaN values for autocorrelation calculation\n",
    "    if len(x) < 2:\n",
    "        return np.nan\n",
    "    return x.autocorr(lag=1)\n",
    "\n",
    "features['lag1_autocorr'] = training_set.apply(lag1_autocorr)\n",
    "\n",
    "print(\"Extracted Features:\")\n",
    "print(features.head())\n",
    "\n",
    "# Step 2: Preprocess the feature set\n",
    "\n",
    "# Fill any remaining NaN values that might be present after feature extraction\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# Standardize the features (important for many clustering algorithms)\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Step 3: Apply clustering\n",
    "\n",
    "# Here we choose k-means as an example. You may adjust the number of clusters as needed.\n",
    "n_clusters = 3  # You can choose the number of clusters based on domain knowledge or via methods like the elbow method.\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "clusters = kmeans.fit_predict(features_scaled)\n",
    "\n",
    "# Add the cluster labels to your features DataFrame\n",
    "features['cluster'] = clusters\n",
    "\n",
    "print(\"\\nCluster assignments:\")\n",
    "print(features[['cluster']].head())\n",
    "\n",
    "# Optional: Visualize the clustering using the first two principal components.\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "features_pca = pca.fit_transform(features_scaled)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(features_pca[:, 0], features_pca[:, 1], c=clusters, cmap='viridis', edgecolor='k', s=100)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('Clustering of Customers based on Time-Series Features')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Suppose 'training_set' is your pandas DataFrame with a datetime index\n",
    "# and each column corresponds to a customer’s power consumption time series.\n",
    "\n",
    "# Step 1: Fill missing values\n",
    "# Here we fill forward then backward; adjust if a different imputation is preferred.\n",
    "df_filled = training_set.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# Step 2: Choose a reference time series.\n",
    "# For instance, here we select the first customer as the reference.\n",
    "ref_customer = df_filled.columns[0]\n",
    "ref_series = df_filled[ref_customer]\n",
    "\n",
    "# Step 3: Compute the Euclidean distance between the reference and each other customer.\n",
    "# This distance is computed over all time points.\n",
    "distances = {}\n",
    "for customer in df_filled.columns:\n",
    "    if customer == ref_customer:\n",
    "        continue  # Skip comparing the reference with itself.\n",
    "    # Calculate Euclidean distance\n",
    "    distances[customer] = np.linalg.norm(ref_series - df_filled[customer])\n",
    "\n",
    "# Step 4: Select the four customers that are closest to the reference.\n",
    "closest_customers = sorted(distances, key=distances.get)[:4]\n",
    "\n",
    "# Include the reference customer in the list of most similar paths.\n",
    "selected_customers = [ref_customer] + closest_customers\n",
    "\n",
    "print(\"Selected customers:\", selected_customers)\n",
    "\n",
    "# Step 5: Plot the selected five time series together.\n",
    "df_filled = df_filled[-200:]\n",
    "plt.figure(figsize=(12, 6))\n",
    "for customer in selected_customers:\n",
    "    plt.plot(df_filled.index, df_filled[customer], label=customer)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Power Consumption\")\n",
    "plt.title(\"Five Most Similar Customer Paths\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import esig as ts\n",
    "\n",
    "# Assuming training_set is a DataFrame with datetime index and customer columns\n",
    "# Transpose: now each row is a time series of a customer\n",
    "training_set = training_set[training_set.columns[:300]]\n",
    "training_set = training_set[-10_000:].fillna(0)\n",
    "customer_series = training_set.T\n",
    "\n",
    "\n",
    "def compute_signatures(series_df, depth=2):\n",
    "    \"\"\"\n",
    "    Compute the signature of each customer's time series.\n",
    "    Each row of series_df is a time series for one customer.\n",
    "    \"\"\"\n",
    "    signatures = []\n",
    "    for _, path in series_df.iterrows():\n",
    "        # Convert to array and reshape (T, 1) to be a 1D path\n",
    "        path_array = path.to_numpy().reshape(-1, 1)\n",
    "        sig = ts.stream2sig(path_array, depth)\n",
    "        signatures.append(sig)\n",
    "    return np.vstack(signatures)\n",
    "\n",
    "sig_features = compute_signatures(customer_series, depth=10)\n",
    "\n",
    "# Compute the kernel matrix (Gram matrix)\n",
    "kernel_matrix = np.dot(sig_features, sig_features.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "# 4. Run Kernel PCA with 2 components\n",
    "kpca = KernelPCA(kernel='precomputed', n_components=2)\n",
    "X_kpca = kpca.fit_transform(kernel_matrix)\n",
    "\n",
    "# 5. Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_kpca[:, 0], X_kpca[:, 1], c='steelblue', edgecolor='k', s=80)\n",
    "plt.title(\"Kernel PCA (Signature Kernel) - 2 Components\")\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## naive improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class TimeOLSmodel:\n",
    "    def __init__(self, prediction_window: int = 720):\n",
    "        self.prediction_window = prediction_window\n",
    "        self.model = LinearRegression()\n",
    "        self.encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "    def _generate_time_features(self, timestamps: pd.Series):\n",
    "        \"\"\"\n",
    "        Generate one-hot encoded hour of day and day of week features from timestamps.\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame()\n",
    "        df[\"hour\"] = timestamps.dt.hour\n",
    "        df[\"dayofweek\"] = timestamps.dt.dayofweek\n",
    "        return self.encoder.transform(df)\n",
    "\n",
    "    def train(self, x: pd.DataFrame, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        x should include a 'timestamp' (datetime64) column and a 'consumption' column.\n",
    "        \"\"\"\n",
    "        if not pd.api.types.is_datetime64_any_dtype(x[\"timestamp\"]):\n",
    "            x[\"timestamp\"] = pd.to_datetime(x[\"timestamp\"])\n",
    "\n",
    "        # Fit the encoder on the time features\n",
    "        df = pd.DataFrame()\n",
    "        df[\"hour\"] = x[\"timestamp\"].dt.hour\n",
    "        df[\"dayofweek\"] = x[\"timestamp\"].dt.dayofweek\n",
    "        self.encoder.fit(df)\n",
    "\n",
    "        time_features = self._generate_time_features(x[\"timestamp\"])\n",
    "        \n",
    "        # Combine with past consumption as feature\n",
    "        features = np.column_stack([x[\"consumption\"].values.reshape(-1, 1), time_features.values])\n",
    "\n",
    "        self.model.fit(features, y)\n",
    "\n",
    "    def predict(self, x: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        x should include a 'timestamp' (datetime64) column and a 'consumption' column.\n",
    "        \"\"\"\n",
    "        if not pd.api.types.is_datetime64_any_dtype(x[\"timestamp\"]):\n",
    "            x[\"timestamp\"] = pd.to_datetime(x[\"timestamp\"])\n",
    "\n",
    "        time_features = self._generate_time_features(x[\"timestamp\"])\n",
    "        features = np.column_stack([x[\"consumption\"].values.reshape(-1, 1), time_features.values])\n",
    "\n",
    "        return self.model.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
