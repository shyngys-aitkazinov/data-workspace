{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections.abc import Callable\n",
    "from os.path import join\n",
    "from typing import Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# depending on your IDE, you might need to add datathon_eth. in front of data\n",
    "from data import DataLoader, DatasetEncoding, SimpleEncoding\n",
    "\n",
    "# depending on your IDE, you might need to add datathon_eth. in front of forecast_models\n",
    "from forecast_models import SimpleModel, elastic_net_predictor\n",
    "\n",
    "\n",
    "def evaluate_forecast(y_true, y_pred):\n",
    "    diff = y_pred - y_true\n",
    "    country_error = diff.abs().sum()\n",
    "    portfolio_country_error = diff.sum()\n",
    "    return country_error, abs(portfolio_country_error)\n",
    "\n",
    "\n",
    "def cross_validate_forecaster(\n",
    "    predictor: Callable,\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    verbose=True,\n",
    "    save_path=None,\n",
    "    n_splits=5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform time-series cross-validation.\n",
    "    Returns:\n",
    "      mean_abs_err, mean_port_err, mean_final_score,\n",
    "      std_abs_err,  std_port_err,  std_final_score\n",
    "    \"\"\"\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    all_absolute_errors = []\n",
    "    all_portfolio_errors = []\n",
    "    fold_scores = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tscv.split(y.index), start=1):\n",
    "        train_dates = pd.to_datetime(y.index[train_idx])\n",
    "        test_dates = pd.to_datetime(y.index[test_idx])\n",
    "\n",
    "        if len(test_dates) == 0:\n",
    "            continue\n",
    "\n",
    "        y_train = y.loc[train_dates]\n",
    "        y_test = y.loc[test_dates]\n",
    "\n",
    "        X_train = X.loc[train_dates]\n",
    "        X_test = X.loc[test_dates]\n",
    "\n",
    "        y_hat = predictor(X_train, y_train, X_test)\n",
    "\n",
    "        country_err, portfolio_err = evaluate_forecast(y_test, y_hat)\n",
    "\n",
    "        # Sum of absolute errors for that fold\n",
    "        mean_fold_abs = country_err\n",
    "        mean_fold_port = portfolio_err\n",
    "\n",
    "        # Example scoring formula (your logic may differ):\n",
    "        final_fold_score = (\n",
    "            1.0 * mean_fold_abs\n",
    "            + 5.0 * mean_fold_abs\n",
    "            + 10.0 * mean_fold_port\n",
    "            + 50.0 * mean_fold_port\n",
    "        )\n",
    "        # => 6.0 * mean_fold_abs + 60.0 * mean_fold_port\n",
    "\n",
    "        all_absolute_errors.append(mean_fold_abs)\n",
    "        all_portfolio_errors.append(mean_fold_port)\n",
    "        fold_scores.append(final_fold_score)\n",
    "\n",
    "    # Final metrics across folds\n",
    "    mean_abs_err = np.mean(all_absolute_errors)\n",
    "    mean_port_err = np.mean(all_portfolio_errors)\n",
    "    mean_final_score = np.mean(fold_scores)\n",
    "\n",
    "    std_abs_err = np.std(all_absolute_errors)\n",
    "    std_port_err = np.std(all_portfolio_errors)\n",
    "    std_final_score = np.std(fold_scores)\n",
    "\n",
    "    return (\n",
    "        mean_abs_err,\n",
    "        mean_port_err,\n",
    "        mean_final_score,\n",
    "        std_abs_err,\n",
    "        std_port_err,\n",
    "        std_final_score,\n",
    "    )\n",
    "\n",
    "\n",
    "def evaluate(X: pd.DataFrame, y: pd.Series, save_path: str, my_predictor):\n",
    "    \"\"\"\n",
    "    Runs cross-validation with a given predictor.\n",
    "    Returns:\n",
    "      abs_err, port_err, score, abs_err_std, port_err_std, score_std\n",
    "    \"\"\"\n",
    "    results = cross_validate_forecaster(\n",
    "        predictor=my_predictor,\n",
    "        X=X,\n",
    "        y=y,\n",
    "        verbose=True,\n",
    "        save_path=save_path,  # not currently used to save anything, but left for clarity\n",
    "    )\n",
    "\n",
    "    (abs_err,\n",
    "     port_err,\n",
    "     score,\n",
    "     abs_err_std,\n",
    "     port_err_std,\n",
    "     score_std) = results\n",
    "\n",
    "    return abs_err, port_err, score, abs_err_std, port_err_std, score_std\n",
    "\n",
    "\n",
    "def main(model_name: str, my_predictor, max_customers=10):\n",
    "    \"\"\"\n",
    "    Train and evaluate the models for IT and ES,\n",
    "    then store one final score (sum of ES and IT),\n",
    "    and also store the std dev of all absolute errors across both zones.\n",
    "    \"\"\"\n",
    "\n",
    "    # We'll accumulate each zone's results in memory\n",
    "    zone_final_scores = {}\n",
    "    zone_errors_dataframes = {}\n",
    "\n",
    "    for zone in [\"ES\", \"IT\"]:\n",
    "        # Inputs\n",
    "        input_path = r\"datasets2025\"\n",
    "        output_path = r\"outputs\"\n",
    "\n",
    "        # Load Datasets\n",
    "        loader = DataLoader(input_path)\n",
    "        training_set, features, example_results = loader.load_data(zone)\n",
    "\n",
    "        # Additional data\n",
    "        rollout, holidays = loader.load_additional_data(zone)\n",
    "\n",
    "        # Data Manipulation and Training\n",
    "        end_training = training_set.index.max()\n",
    "        start_forecast, end_forecast = example_results.index[0], example_results.index[-1]\n",
    "\n",
    "        dataset_encoding = DatasetEncoding(\n",
    "            training_set,\n",
    "            features,\n",
    "            rollout,\n",
    "            holidays,\n",
    "            end_training=end_training,\n",
    "            start_forecast=start_forecast,\n",
    "            end_forecast=end_forecast,\n",
    "        )\n",
    "\n",
    "        range_forecast = pd.date_range(start=start_forecast, end=end_forecast, freq=\"1H\")\n",
    "        forecast = pd.DataFrame(columns=training_set.columns, index=range_forecast)\n",
    "        forecast_step = 1\n",
    "\n",
    "        # We'll store one row per-customer\n",
    "        errors = pd.DataFrame(\n",
    "            columns=[\n",
    "                \"abs_err\",\n",
    "                \"port_err\",\n",
    "                \"abs_err_std\",\n",
    "                \"port_err_std\",\n",
    "                # We'll add \"raw_cv_score\" and \"raw_cv_score_std\" if we want to keep them\n",
    "                \"cv_score\",\n",
    "                \"cv_score_std\",\n",
    "                \"country\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        customers = training_set.columns.values[:max_customers]\n",
    "\n",
    "        for i, costumer in enumerate(customers, start=1):\n",
    "            customer_id = int(costumer.split(\"_\")[-1])\n",
    "            print(customer_id)\n",
    "            # Progress bar\n",
    "            bar_length = 30\n",
    "            progress = int(bar_length * i / max_customers)\n",
    "            bar = \"#\" * progress + \"-\" * (bar_length - progress)\n",
    "            print(f\"\\r[{bar}] {i}/{max_customers} customers processed\", end=\"\", flush=True)\n",
    "\n",
    "            df = dataset_encoding.generate_dataset(\n",
    "                customer_id,\n",
    "                window_size=24 * 7,\n",
    "                forecast_skip=1,\n",
    "                forecast_horizon=1,\n",
    "                additional_feats=[\"mean\", \"std\", \"skew\", \"kurtosis\", \"min\", \"max\"],\n",
    "            )\n",
    "\n",
    "            # Evaluate\n",
    "            X, y = dataset_encoding.get_train_data(\n",
    "                df, customer_id, forecast_step=forecast_step, drop_nans_X=True\n",
    "            )\n",
    "\n",
    "            (\n",
    "                abs_err,\n",
    "                port_err,\n",
    "                cv_score,\n",
    "                abs_err_std,\n",
    "                port_err_std,\n",
    "                cv_score_std,\n",
    "            ) = evaluate(X, y, f\"{output_path}/{customer_id}.png\", my_predictor)\n",
    "\n",
    "            errors.loc[customer_id] = {\n",
    "                \"abs_err\": abs_err,\n",
    "                \"port_err\": port_err,\n",
    "                \"abs_err_std\": abs_err_std,\n",
    "                \"port_err_std\": port_err_std,\n",
    "                \"cv_score\": cv_score,\n",
    "                \"cv_score_std\": cv_score_std,\n",
    "                \"country\": zone,\n",
    "            }\n",
    "\n",
    "        # Now compute final \"score\" per-customer with the zone-specific weighting\n",
    "        # (Stays the same as before, purely example logic).\n",
    "        errors[\"score\"] = errors.apply(\n",
    "            lambda x: (x[\"abs_err\"] * 5 + x[\"port_err\"] * 50)\n",
    "            if x[\"country\"] == \"ES\"\n",
    "            else (x[\"abs_err\"] * 1 + x[\"port_err\"] * 10),\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        # Save the CSV for this zone\n",
    "        errors.to_csv(f\"{output_path}/errors_{zone}.csv\")\n",
    "\n",
    "        # Final \"average score\" across all customers for THIS zone\n",
    "        final_score_zone = float(errors[\"score\"].mean())\n",
    "        zone_final_scores[zone] = final_score_zone\n",
    "\n",
    "        # Keep the DataFrame in memory for post-processing\n",
    "        zone_errors_dataframes[zone] = errors\n",
    "\n",
    "        print(f\"\\n\\n===== ZONE: {zone} =====\")\n",
    "        print(f\"Final average score: {final_score_zone:.2f}\")\n",
    "        print(\"========================\\n\")\n",
    "\n",
    "    # -----------------------\n",
    "    # After processing both zones, we compute:\n",
    "    #  1) A single final score = sum of the ES and IT final scores\n",
    "    #  2) The overall std of all absolute errors across *both* zones\n",
    "    # -----------------------\n",
    "    final_score = zone_final_scores[\"ES\"] + zone_final_scores[\"IT\"]\n",
    "    combined_errors = pd.concat(zone_errors_dataframes.values())\n",
    "\n",
    "    # \"The standard deviation of all errors\" typically means the std of abs_err across the entire dataset\n",
    "    overall_std_abs_err = float(combined_errors[\"abs_err\"].std())\n",
    "\n",
    "    # Build one single record for the JSON\n",
    "    record = {\n",
    "        \"model\": model_name,\n",
    "        # single final score (sum of zone-specific final scores)\n",
    "        \"final_score\": final_score,\n",
    "        # standard deviation of all absolute errors across ES & IT\n",
    "        \"std_abs_err\": overall_std_abs_err,\n",
    "    }\n",
    "\n",
    "    # You might also decide to store \"std of score\" or portfolio errors here.\n",
    "    # For example:\n",
    "    # record[\"std_score\"] = float(combined_errors[\"score\"].std())\n",
    "\n",
    "    # Append to results.json (as one line)\n",
    "    with open(\"results.json\", \"a\", encoding=\"utf-8\") as f:\n",
    "        json.dump(record, f)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "    print(\"===== FINAL COMBINED =====\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Final Summed Score (ES+IT): {final_score:.2f}\")\n",
    "    print(f\"Std of all absolute errors: {overall_std_abs_err:.2f}\")\n",
    "    print(\"========================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[------------------------------] 1/100 customers processed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3b/v9q3pbyx1r1gb74yk77xgx900000gq/T/ipykernel_15633/1138426840.py:158: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  range_forecast = pd.date_range(start=start_forecast, end=end_forecast, freq=\"1H\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[------------------------------] 2/100 customers processed5\n",
      "[------------------------------] 3/100 customers processed11\n",
      "[#-----------------------------] 4/100 customers processed19\n",
      "[#-----------------------------] 5/100 customers processed30\n",
      "[#-----------------------------] 6/100 customers processed31\n",
      "[##----------------------------] 7/100 customers processed39\n",
      "[##----------------------------] 8/100 customers processed40\n",
      "[##----------------------------] 9/100 customers processed44\n",
      "[###---------------------------] 10/100 customers processed45\n",
      "[###---------------------------] 11/100 customers processed46\n",
      "[###---------------------------] 12/100 customers processed48\n",
      "[###---------------------------] 13/100 customers processed59\n",
      "[####--------------------------] 14/100 customers processed60\n",
      "[####--------------------------] 15/100 customers processed61\n",
      "[####--------------------------] 16/100 customers processed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexanders/datathon_2025/.venv/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/alexanders/datathon_2025/.venv/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/alexanders/datathon_2025/.venv/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/alexanders/datathon_2025/.venv/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/alexanders/datathon_2025/.venv/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n",
      "[#####-------------------------] 17/100 customers processed63\n",
      "[#####-------------------------] 18/100 customers processed64\n",
      "[#####-------------------------] 19/100 customers processed66\n",
      "[######------------------------] 20/100 customers processed67\n",
      "[######------------------------] 21/100 customers processed69\n",
      "[######------------------------] 22/100 customers processed70\n",
      "[######------------------------] 23/100 customers processed72\n",
      "[#######-----------------------] 24/100 customers processed74\n",
      "[#######-----------------------] 25/100 customers processed76\n",
      "[#######-----------------------] 26/100 customers processed80\n",
      "[########----------------------] 27/100 customers processed89\n",
      "[########----------------------] 28/100 customers processed90\n",
      "[########----------------------] 29/100 customers processed92\n",
      "[#########---------------------] 30/100 customers processed93\n",
      "[#########---------------------] 31/100 customers processed94\n",
      "[#########---------------------] 32/100 customers processed96\n",
      "[#########---------------------] 33/100 customers processed97\n",
      "[##########--------------------] 34/100 customers processed98\n",
      "[##########--------------------] 35/100 customers processed100\n",
      "[##########--------------------] 36/100 customers processed"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "100",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melastic_net_predictor\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melastic_net_predictor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 187\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(model_name, my_predictor, max_customers)\u001b[0m\n\u001b[1;32m    184\u001b[0m bar \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m progress \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m (bar_length \u001b[38;5;241m-\u001b[39m progress)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbar\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_customers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m customers processed\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 187\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_encoding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustomer_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforecast_skip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforecast_horizon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43madditional_feats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mskew\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkurtosis\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[1;32m    196\u001b[0m X, y \u001b[38;5;241m=\u001b[39m dataset_encoding\u001b[38;5;241m.\u001b[39mget_train_data(\n\u001b[1;32m    197\u001b[0m     df, customer_id, forecast_step\u001b[38;5;241m=\u001b[39mforecast_step, drop_nans_X\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    198\u001b[0m )\n",
      "File \u001b[0;32m~/datathon_2025/data.py:250\u001b[0m, in \u001b[0;36mDatasetEncoding.generate_dataset\u001b[0;34m(self, customer_id, window_size, forecast_skip, forecast_horizon, start_time, end_time, include_time_features, additional_feats)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03mGenerate a supervised dataset for a single customer.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;124;03m                  - Future consumption values (forecast_horizon columns)\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# 1) Identify the target series for the chosen customer\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m cust_col_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcustomer_id_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcustomer_id\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    251\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconsumption[cust_col_name]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# If no start/end time provided, fallback to entire range for that customer\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 100"
     ]
    }
   ],
   "source": [
    "model_name = \"elastic_net_predictor\"\n",
    "main(model_name, elastic_net_predictor, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
